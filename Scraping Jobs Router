from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks, Query
from sqlalchemy.orm import Session
from typing import List, Optional
import logging

from app.api.deps import get_db, get_current_user
from app.schemas.job import ScrapingJobCreate, ScrapingJobResponse, ScrapingJobSummary
from app.services.scraper import ScraperService
from app.models.user import User

router = APIRouter()
logger = logging.getLogger(__name__)

@router.post("/", response_model=ScrapingJobResponse)
async def create_scraping_job(
    job: ScrapingJobCreate,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Create a new scraping job"""
    logger.info(f"Creating new scraping job: {job.city}, {job.state}")
    
    scraper_service = ScraperService(db)
    scraping_job = scraper_service.create_job(job, current_user.id)
    
    # Start the job in the background
    background_tasks.add_task(scraper_service.run_job, scraping_job.id)
    
    return scraping_job

@router.get("/", response_model=List[ScrapingJobSummary])
async def list_scraping_jobs(
    skip: int = 0,
    limit: int = 100,
    status: Optional[str] = None,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """List all scraping jobs"""
    scraper_service = ScraperService(db)
    jobs = scraper_service.get_jobs(skip=skip, limit=limit, status=status)
    return jobs

@router.get("/{job_id}", response_model=ScrapingJobResponse)
async def get_scraping_job(
    job_id: str,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Get a specific scraping job by ID"""
    scraper_service = ScraperService(db)
    job = scraper_service.get_job(job_id)
    
    if not job:
        raise HTTPException(status_code=404, detail="Scraping job not found")
    
    return job

@router.post("/{job_id}/cancel", response_model=ScrapingJobResponse)
async def cancel_scraping_job(
    job_id: str,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Cancel a running scraping job"""
    scraper_service = ScraperService(db)
    job = scraper_service.cancel_job(job_id)
    
    if not job:
        raise HTTPException(status_code=404, detail="Scraping job not found")
    
    return job

@router.post("/{job_id}/retry", response_model=ScrapingJobResponse)
async def retry_scraping_job(
    job_id: str,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Retry a failed scraping job"""
    scraper_service = ScraperService(db)
    job = scraper_service.retry_job(job_id)
    
    if not job:
        raise HTTPException(status_code=404, detail="Scraping job not found")
    
    # Start the job in the background
    background_tasks.add_task(scraper_service.run_job, job.id)
    
    return job

@router.get("/{job_id}/progress", response_model=dict)
async def get_job_progress(
    job_id: str,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Get detailed progress of a scraping job"""
    scraper_service = ScraperService(db)
    progress = scraper_service.get_job_progress(job_id)
    
    if not progress:
        raise HTTPException(status_code=404, detail="Scraping job not found")
    
    return progress
